# vllm-redhat-llama8b.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-llama8b
  namespace: vllm
spec:
  replicas: 1
  selector:
    matchLabels: { app: vllm-llama8b }
  template:
    metadata:
      labels: { app: vllm-llama8b }
    spec:
      nodeSelector:
        kubernetes.azure.com/agentpool: "gpua10"
      tolerations:
        - key: "sku"
          operator: "Equal"
          value: "gpu"
          effect: "NoSchedule"
      containers:
        - name: vllm
          image: registry.access.redhat.com/rhaiis/rh-vllm-cuda
          # Equivalent to: vllm serve --model ... (see args below)
          command: ["vllm"]
          args:
            - "serve"
            - "--model"
            - "RedHatAI/Llama-3.1-8B-Instruct"
            - "--tensor-parallel-size"
            - "1"            # single A10 GPU
            - "--dtype"
            - "bfloat16"
            - "--max-model-len"
            - "8192"         # use 4096 if you see OOM
            - "--enforce-eager"
            - "--port"
            - "8000"
          env:
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf
                  key: HUGGING_FACE_HUB_TOKEN
            - name: HF_HUB_OFFLINE
              value: "0"
            - name: HF_HOME               # make HF cache live on the PVC
              value: "/home/vllm/.cache"
            - name: VLLM_WORKER_GPU_MEMORY_UTILIZATION
              value: "0.95"              # lower to 0.90 if tight on VRAM
          ports:
            - name: http
              containerPort: 8000
          resources:
            requests:
              nvidia.com/gpu: 1
            limits:
              nvidia.com/gpu: 1
          volumeMounts:
            - name: cache
              mountPath: /home/vllm/.cache
          readinessProbe:
            httpGet:
              path: /v1/models
              port: http
            initialDelaySeconds: 10
            periodSeconds: 10
            failureThreshold: 24
      volumes:
        - name: cache
          persistentVolumeClaim:
            claimName: vllm-cache
---
apiVersion: v1
kind: Service
metadata:
  name: vllm-llama8b
  namespace: vllm
spec:
  type: LoadBalancer
  selector: { app: vllm-llama8b }
  ports:
    - name: http
      port: 8000
      targetPort: http
