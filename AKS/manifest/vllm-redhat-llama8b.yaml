apiVersion: v1
kind: Namespace
metadata:
  name: vllm
---
# Hugging Face token (REQUIRED). Replace the placeholder with your real token
# apiVersion: v1
# kind: Secret
# metadata:
#   name: hf
#   namespace: vllm
# type: Opaque
# stringData:
#   HUGGING_FACE_HUB_TOKEN: "<PUT_YOUR_HF_TOKEN_HERE>"
---
# Persistent cache so the model isn't re-downloaded on every restart
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: vllm-cache
  namespace: vllm
spec:
  accessModes: ["ReadWriteOnce"]
  resources:
    requests:
      storage: 50Gi
  storageClassName: default
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-llama8b
  namespace: vllm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-llama8b
  template:
    metadata:
      labels:
        app: vllm-llama8b
    spec:
      # If your registry.redhat.io requires auth, uncomment and create a docker-registry secret named rh-pull
      # imagePullSecrets:
      #   - name: rh-pull

      # Ensure PVC mounts are group-writable; keep fsGroup but don't force a non-existent UID
      securityContext:
        fsGroup: 1000
        fsGroupChangePolicy: OnRootMismatch

      # Schedule on your GPU pool and tolerate its taint
      nodeSelector:
        kubernetes.azure.com/agentpool: "gpua10"
      tolerations:
        - key: "sku"
          operator: "Equal"
          value: "gpu"
          effect: "NoSchedule"

      # Fix permissions on cache before vLLM starts (handles any root-owned leftovers)
      initContainers:
        - name: fix-cache-perms
          image: registry.access.redhat.com/ubi9/ubi-minimal:latest
          command: ["/bin/sh", "-lc"]
          args:
            - |
              mkdir -p /home/vllm/.cache \
              && chown -R 1000:1000 /home/vllm/.cache || true
          volumeMounts:
            - name: cache
              mountPath: /home/vllm/.cache
          securityContext:
            runAsUser: 0

      containers:
        - name: vllm
          image: registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.1
          command: ["vllm"]
          args:
            - "serve"
            - "RedHatAI/Llama-3.1-8B-Instruct"  # positional (no --model)
            - "--tensor-parallel-size"
            - "1"              # single A10
            - "--dtype"
            - "bfloat16"
            - "--max-model-len"
            - "8192"           # drop to 4096 if memory is tight
            - "--enforce-eager"
            - "--port"
            - "8000"
          env:
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf
                  key: HUGGING_FACE_HUB_TOKEN
            - name: HF_HUB_OFFLINE
              value: "0"
            - name: HF_HOME
              value: "/home/vllm/.cache"
            - name: TORCHINDUCTOR_CACHE_DIR
              value: "/tmp/torchinductor"
            - name: VLLM_WORKER_GPU_MEMORY_UTILIZATION
              value: "0.85"    # try 0.90 if OOM
          # Run as root to avoid getpwuid() crash from PyTorch Inductor
          securityContext:
            runAsUser: 0
          resources:
            requests:
              nvidia.com/gpu: 1
            limits:
              nvidia.com/gpu: 1
          ports:
            - name: http
              containerPort: 8000
          volumeMounts:
            - name: cache
              mountPath: /home/vllm/.cache
          readinessProbe:
            httpGet:
              path: /v1/models
              port: http
            initialDelaySeconds: 20
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 30
      volumes:
        - name: cache
          persistentVolumeClaim:
            claimName: vllm-cache
---
apiVersion: v1
kind: Service
metadata:
  name: vllm-llama8b
  namespace: vllm
spec:
  type: LoadBalancer
  selector:
    app: vllm-llama8b
  ports:
    - name: http
      port: 8000
      targetPort: http
