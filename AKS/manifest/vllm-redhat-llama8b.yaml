apiVersion: v1
kind: Namespace
metadata:
  name: vllm
---
# Optional: Red Hat registry pull secret (uncomment & fill if your RH image needs auth)
# apiVersion: v1
# kind: Secret
# metadata:
#   name: rh-pull
#   namespace: vllm
# type: kubernetes.io/dockerconfigjson
# data:
#   .dockerconfigjson: <base64 of your docker config json>

---
# Model/cache PVC so downloads persist across restarts
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: vllm-cache
  namespace: vllm
spec:
  accessModes: ["ReadWriteOnce"]
  resources:
    requests:
      storage: 50Gi
  storageClassName: default
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-llama8b
  namespace: vllm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-llama8b
  template:
    metadata:
      labels:
        app: vllm-llama8b
    spec:
      # Uncomment if your RH image requires auth from registry.redhat.io
      # imagePullSecrets:
      #   - name: rh-pull

      # Ensure the mounted PVC is writable by the container user
      securityContext:
        fsGroup: 1000
        fsGroupChangePolicy: OnRootMismatch

      # GPU nodepool scheduling (matches your AKS setup)
      nodeSelector:
        kubernetes.azure.com/agentpool: "gpua10"
      tolerations:
        - key: "sku"
          operator: "Equal"
          value: "gpu"
          effect: "NoSchedule"

      # Pre-fix permissions on the cache volume (handles leftover root-owned files)
      initContainers:
        - name: fix-cache-perms
          image: registry.access.redhat.com/ubi9/ubi-minimal:latest
          command: ["/bin/sh", "-lc"]
          args:
            - |
              mkdir -p /home/vllm/.cache && chown -R 1000:1000 /home/vllm/.cache || true
          volumeMounts:
            - name: cache
              mountPath: /home/vllm/.cache
          securityContext:
            runAsUser: 0

      containers:
        - name: vllm
          image: registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.1
          command: ["vllm"]
          args:
            - "serve"
            - "RedHatAI/Llama-3.1-8B-Instruct"  # positional arg (no --model)
            - "--tensor-parallel-size"
            - "1"              # single A10
            - "--dtype"
            - "bfloat16"
            - "--max-model-len"
            - "8192"           # use 4096 if memory is tight
            - "--enforce-eager"
            - "--port"
            - "8000"
          env:
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf
                  key: HUGGING_FACE_HUB_TOKEN
            - name: HF_HUB_OFFLINE
              value: "0"
            - name: HF_HOME
              value: "/home/vllm/.cache"
            - name: VLLM_WORKER_GPU_MEMORY_UTILIZATION
              value: "0.95"    # if OOM, drop to 0.90
          securityContext:
            runAsNonRoot: true
            runAsUser: 1000
            runAsGroup: 1000
          resources:
            requests:
              nvidia.com/gpu: 1
            limits:
              nvidia.com/gpu: 1
          ports:
            - name: http
              containerPort: 8000
          volumeMounts:
            - name: cache
              mountPath: /home/vllm/.cache
          readinessProbe:
            httpGet:
              path: /v1/models
              port: http
            initialDelaySeconds: 10
            periodSeconds: 10
            failureThreshold: 24
      volumes:
        - name: cache
          persistentVolumeClaim:
            claimName: vllm-cache
---
apiVersion: v1
kind: Service
metadata:
  name: vllm-llama8b
  namespace: vllm
spec:
  type: LoadBalancer
  selector:
    app: vllm-llama8b
  ports:
    - name: http
      port: 8000
      targetPort: http
